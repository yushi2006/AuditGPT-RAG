import os

import streamlit as st
from dotenv import load_dotenv
from langchain.chains import ConversationalRetrievalChain

# Import document loaders
from langchain.document_loaders import (
    Docx2txtLoader,
    PyPDFLoader,
    TextLoader,
    UnstructuredExcelLoader,
)
from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import Chroma

# Import LanchChain components
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# --- SETUP ---
# ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© (Ù…Ø«Ù„ Ù…ÙØªØ§Ø­ OpenAI API)
load_dotenv()

# --- HELPER FUNCTIONS ---


def process_documents(uploaded_files):
    """
    Processes uploaded documents by loading, splitting, and embedding them.
    ÙŠØ¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ù…ÙŠÙ„Ù‡Ø§ ÙˆØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡Ø§Øª
    """
    all_chunks = []
    temp_dir = "temp_docs"
    if not os.path.exists(temp_dir):
        os.makedirs(temp_dir)

    for uploaded_file in uploaded_files:
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø¤Ù‚ØªØ§Ù‹ Ù„Ù‚Ø±Ø§Ø¡ØªÙ‡
        temp_path = os.path.join(temp_dir, uploaded_file.name)
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù€ Loader Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù…ØªØ¯Ø§Ø¯ Ø§Ù„Ù…Ù„Ù
        if uploaded_file.name.endswith(".pdf"):
            loader = PyPDFLoader(temp_path)
        elif uploaded_file.name.endswith(".docx"):
            loader = Docx2txtLoader(temp_path)
        elif uploaded_file.name.endswith(".xlsx"):
            # Ù…Ù„Ø§Ø­Ø¸Ø©: UnstructuredExcelLoader Ù‚Ø¯ Ù„Ø§ ÙŠÙƒÙˆÙ† Ù…Ø«Ø§Ù„ÙŠØ§Ù‹ Ù„Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
            loader = UnstructuredExcelLoader(temp_path, mode="elements")
        elif uploaded_file.name.endswith(".txt"):
            loader = TextLoader(temp_path)
        else:
            # ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ù…Ø¹ Ø¥Ø´Ø¹Ø§Ø±
            st.warning(f"File type for '{uploaded_file.name}' not supported. Skipping.")
            continue

        # ØªØ­Ù…ÙŠÙ„ ÙˆØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø³ØªÙ†Ø¯
        documents = loader.load_and_split()
        all_chunks.extend(documents)

        # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        os.remove(temp_path)

    if not all_chunks:
        return None

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª (Embeddings) ÙˆØªØ®Ø²ÙŠÙ†Ù‡Ø§ ÙÙŠ ChromaDB
    embeddings = OpenAIEmbeddings(api_key=os.getenv("API_KEY"))
    vectorstore = Chroma.from_documents(documents=all_chunks, embedding=embeddings)

    return vectorstore


def get_conversation_chain(vectorstore):
    """
    Creates and returns a conversational retrieval chain.
    ÙŠÙ†Ø´Ø¦ ÙˆÙŠØ¹ÙŠØ¯ Ø³Ù„Ø³Ù„Ø© Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ø³ØªØ±Ø¬Ø§Ø¹ÙŠØ©
    """
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.3)

    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© Ù„Ø­ÙØ¸ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm, retriever=vectorstore.as_retriever(), memory=memory
    )
    return conversation_chain


# --- STREAMLIT UI ---


def main():
    st.set_page_config(
        page_title="ğŸ¤– Ø±ÙˆØ¨ÙˆØª Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ", page_icon=":guardsman:", layout="wide"
    )

    # ØªÙ‡ÙŠØ¦Ø© Ø­Ø§Ù„Ø© Ø§Ù„Ø¬Ù„Ø³Ø© (Session State) Ù„Ø­ÙØ¸ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "documents_processed" not in st.session_state:
        st.session_state.documents_processed = False

    # --- Sidebar for Document Upload ---
    with st.sidebar:
        st.header("Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ø°ÙƒÙŠ")
        st.write("Ø§Ø±ÙØ¹ Ù…Ø³ØªÙ†Ø¯Ø§ØªÙƒ (PDF, Word, Excel, TXT) Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.")

        uploaded_files = st.file_uploader(
            "Ø§Ø®ØªØ± Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚",
            type=["pdf", "docx", "xlsx", "txt"],
            accept_multiple_files=True,
        )

        if st.button("Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"):
            if uploaded_files:
                with st.spinner("â³ Ø¬Ø§Ø±Ù Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ‚Øª"):
                    # 1. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡ÙŠØ©
                    vectorstore = process_documents(uploaded_files)

                    if vectorstore:
                        # 2. Ø¥Ù†Ø´Ø§Ø¡ Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
                        st.session_state.conversation = get_conversation_chain(
                            vectorstore
                        )
                        st.session_state.documents_processed = True
                        st.success(
                            "âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ù†Ø¬Ø§Ø­! ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©."
                        )
                    else:
                        st.error(
                            "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù‚Ø§Ø¨Ù„ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©."
                        )
            else:
                st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø±ÙØ¹ Ù…Ù„Ù ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„.")

    # --- Main Chat Interface ---
    st.header("ğŸ¤– Ø±ÙˆØ¨ÙˆØª Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ")
    st.write("Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¨Ø³Ø±Ø¹Ø© Ø¯Ø§Ø®Ù„ Ù…Ø³ØªÙ†Ø¯Ø§ØªÙƒ.")
    st.markdown("---")

    # Ø¹Ø±Ø¶ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Ù…Ø±Ø¨Ø¹ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    user_question = st.chat_input("Ø§Ø³Ø£Ù„ Ø¹Ù† Ø§Ù„Ø³ÙŠØ§Ø³Ø§ØªØŒ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§ØªØŒ Ø£Ùˆ Ø§Ø·Ù„Ø¨ ØªÙ„Ø®ÙŠØµØ§Ù‹...")

    if user_question:
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§
        if not st.session_state.documents_processed:
            st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø±ÙØ¹ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø£ÙˆÙ„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ.")
            st.stop()

        # Ø¥Ø¶Ø§ÙØ© Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ¹Ø±Ø¶Ù‡
        st.session_state.chat_history.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.markdown(user_question)

        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø±ÙˆØ¨ÙˆØª
        with st.spinner("ğŸ¤” Ø£ÙÙƒØ±..."):
            response = st.session_state.conversation({"question": user_question})
            bot_response = response["chat_history"][
                -1
            ].content  # Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© Ù‡ÙŠ Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø±ÙˆØ¨ÙˆØª

            # Ø¥Ø¶Ø§ÙØ© Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø±ÙˆØ¨ÙˆØª Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø¬Ù„ ÙˆØ¹Ø±Ø¶Ù‡Ø§
            st.session_state.chat_history.append(
                {"role": "assistant", "content": bot_response}
            )
            with st.chat_message("assistant"):
                st.markdown(bot_response)


if __name__ == "__main__":
    main()
